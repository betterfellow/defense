{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VAE for CIFAR10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.init as init\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "#from tensorboard import SummaryWriter\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size=64\n",
    "learning_rate=0.0003\n",
    "num_epoch=20\n",
    "leak=0.05\n",
    "drop_rate=0.1\n",
    "z_dim=64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CIFAR10 dataset.\n",
    "It has 10 classes: \n",
    "\n",
    "‘airplane’, ‘automobile’, ‘bird’, ‘cat’, ‘deer’,\n",
    "‘dog’, ‘frog’, ‘horse’, ‘ship’, ‘truck’. \n",
    "\n",
    "The images in CIFAR-10 are of\n",
    "size 3x32x32, i.e. 3-channel color images of 32x32 pixels in size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The output of torchvision datasets are PILImage images of range [0, 1].\n",
    "# We transform them to Tensors of normalized range [-1, 1]\n",
    "# Transforms are common image transforms. They can be chained together using `Compose`\n",
    "\n",
    "transform_config = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ../data/cifar-10-python.tar.gz\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Download the data\n",
    "cifar_train=dset.CIFAR10(root=\"../data/\", train=True, transform=transforms.ToTensor(), target_transform=None, download=True)\n",
    "cifar_test=dset.CIFAR10(root=\"../data/\", train=False, transform=transforms.ToTensor(), target_transform=None, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_loader=torch.utils.data.DataLoader(cifar_train,batch_size=batch_size, shuffle=True, num_workers=2,drop_last=True)\n",
    "test_loader=torch.utils.data.DataLoader(cifar_test,batch_size=batch_size,shuffle=False,num_workers=2,drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder\n",
    "* Two outputs\n",
    "    * z_mu\n",
    "    * z_logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Encoder,self).__init__()\n",
    "        \n",
    "        # output size = (image_width - kernel_width)/stride +1\n",
    "        \n",
    "        # torch.nn.Conv2d(in_channels, out_channels, kernel_size,\n",
    "        #                 stride=1, padding=0, dilation=1,\n",
    "        #                 groups=1, bias=True)\n",
    "\n",
    "\n",
    "\n",
    "        self.conv_layer = nn.Sequential(\n",
    "                        nn.Conv2d(3,64,3,padding=1),\n",
    "                        nn.BatchNorm2d(64),\n",
    "                        nn.LeakyReLU(leak),\n",
    "                        nn.Dropout(p=drop_rate,inplace=True),\n",
    "                        # batch_size*  32*32  *64 featuremaps\n",
    "            \n",
    "                        nn.Conv2d(64,128,3,padding=1), \n",
    "                        nn.BatchNorm2d(128),\n",
    "                        nn.LeakyReLU(leak),\n",
    "                        nn.Dropout(p=drop_rate,inplace=True),\n",
    "                        nn.MaxPool2d(2,2),\n",
    "                        # batch_size*  16*16  *128 featuremaps\n",
    "            \n",
    "                        nn.Conv2d(128,256,3,padding=1),\n",
    "                        nn.BatchNorm2d(256),\n",
    "                        nn.LeakyReLU(leak),\n",
    "                        nn.Dropout(p=drop_rate,inplace=True),\n",
    "                        nn.MaxPool2d(2,2),\n",
    "                        # batch_size*  8*8  *256 featuremaps\n",
    "            \n",
    "            \n",
    "                        nn.Conv2d(256,256,3,padding=1,stride=2), # ((8+1*2) - 3)/2 +1 = 4\n",
    "                        nn.BatchNorm2d(256),\n",
    "                        nn.LeakyReLU(leak),\n",
    "                        nn.Dropout(p=drop_rate,inplace=True),\n",
    "                        # batch_size*  4*4  *256 featuremaps\n",
    "                \n",
    "                        nn.Conv2d(256,1024,4, padding=0),  # ((4 - 2)/1 +1 = 28\n",
    "                        nn.BatchNorm2d(1024),\n",
    "                        nn.LeakyReLU(leak)\n",
    "                        # batch_size*  1*1  *1024 featuremaps\n",
    "            \n",
    "        )\n",
    "        \n",
    "        self.fc_layer = nn.Sequential(\n",
    "                        nn.Linear(1*1*1024,512),\n",
    "                        nn.LeakyReLU(leak),\n",
    "                        nn.Dropout(p=drop_rate,inplace=True),\n",
    "                        nn.Linear(512,128),\n",
    "                        nn.LeakyReLU(leak)\n",
    "        )\n",
    "   \n",
    "        # ===== Two output vectors generated by the Encoder =====\n",
    "        \n",
    "        # One for z_mu\n",
    "        self.z_mu=nn.Sequential(\n",
    "            nn.Linear(128,z_dim),\n",
    "            nn.LeakyReLU(leak)\n",
    "        )\n",
    "        \n",
    "        # Another for z_logvar\n",
    "        self.z_logvar=nn.Sequential(\n",
    "            nn.Linear(128,z_dim),\n",
    "            nn.LeakyReLU(leak)\n",
    "        )\n",
    "        \n",
    "        \n",
    "    def forward(self,x):\n",
    "        out=self.conv_layer(x)\n",
    "        out=out.view(batch_size,-1)\n",
    "        \n",
    "        out=self.fc_layer(out)\n",
    "\n",
    "        z_mu=self.z_mu(out)\n",
    "        z_logvar=self.z_logvar(out)\n",
    "        \n",
    "        return z_mu,z_logvar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder\n",
    "* input: sampled_z\n",
    "* output: reconstructed image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# torch.nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride=1, padding=0, \n",
    "#   output_padding=0, groups=1, bias=True, dilation=1)\n",
    "\n",
    "# H_out=(H_in−1)∗stride(4th) + kernel_size(3rd) − 2∗padding(5th) + output_padding\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Decoder,self).__init__()\n",
    "        \n",
    "        self.fc_layer=nn.Sequential(\n",
    "            nn.Linear(z_dim,128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.LeakyReLU(leak),\n",
    "            nn.Dropout(p=drop_rate,inplace=True),\n",
    "            \n",
    "            nn.Linear(128,256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.LeakyReLU(leak),\n",
    "            nn.Dropout(p=drop_rate,inplace=True),\n",
    "            \n",
    "            nn.Linear(256,1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.LeakyReLU(leak),\n",
    "            nn.Dropout(p=drop_rate,inplace=True),\n",
    "            \n",
    "            nn.Linear(1024,512*2*2),\n",
    "            nn.BatchNorm1d(512*2*2),\n",
    "            nn.LeakyReLU(leak)\n",
    "        )\n",
    "        \n",
    "        self.transConv_layer=nn.Sequential(\n",
    "            \n",
    "            nn.ConvTranspose2d(512,256,3,2,1,1), \n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(leak),\n",
    "            # 4*4\n",
    "            \n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.ConvTranspose2d(256,256,3,1,1,0),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(leak),\n",
    "            # 8*8\n",
    "            \n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.ConvTranspose2d(256,128,3,1,1,0),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(leak),\n",
    "            # 16*16\n",
    "            \n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.ConvTranspose2d(128,64,3,1,1,0),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(leak),\n",
    "            # 32*32\n",
    "            \n",
    "            nn.ConvTranspose2d(64,3,3,1,1,0),\n",
    "            nn.BatchNorm2d(3),\n",
    "            nn.Sigmoid()\n",
    "            # 32*32\n",
    "            \n",
    "        )\n",
    "    \n",
    "    # decode the sampled_z\n",
    "    def forward(self,sampled_z):\n",
    "\n",
    "        out=self.fc_layer(sampled_z)\n",
    "        out=out.view(batch_size,-1,2,2)\n",
    "        out=self.transConv_layer(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## check GPU availability here bro\n",
    "use_gpu = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### the Model as a whole\n",
    "* encoder & decoder\n",
    "* sample z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(VAE,self).__init__()\n",
    "        \n",
    "        self.encoder=encoder\n",
    "        self.decoder=decoder\n",
    "    \n",
    "    def sample_z(self, z_mu,z_logvar):\n",
    "        \n",
    "        # sample epsilon ~ N(0, 1)\n",
    "        \n",
    "        epsilon=Variable(torch.randn(batch_size,z_dim),requires_grad=False)\n",
    "        if use_gpu:\n",
    "            epsilon = epsilon.cuda()\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "        # reparameterization trick\n",
    "        sampled_z=z_mu+torch.mul(torch.exp(z_logvar),epsilon)\n",
    "       \n",
    "        return sampled_z\n",
    "        \n",
    "            \n",
    "    def forward(self,x):\n",
    "        # encode the latent variable from the image\n",
    "        # sample z from the encoded result\n",
    "        # decode the sampled z\n",
    "        \n",
    "        z_mu,z_logvar=self.encoder(x)\n",
    "        \n",
    "        self.z_mu=z_mu\n",
    "        self.z_logvar=z_logvar\n",
    "\n",
    "        sampled_z=self.sample_z(z_mu,z_logvar)\n",
    "\n",
    "        result=self.decoder(sampled_z)\n",
    "        \n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check GPU availibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use CPU\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if use_gpu:\n",
    "    model = VAE(Encoder(),Decoder()).cuda()\n",
    "    print ('Use GPU')\n",
    "else:\n",
    "    model = VAE(Encoder(),Decoder())\n",
    "    print ('Use CPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loss_func(generated_image,x,z_mu,z_logvar):\n",
    "    \n",
    "    recon=nn.BCELoss(size_average=False)\n",
    "    reconstruction_loss=recon(generated_image,x)\n",
    "    \n",
    "    # alias the KL-divergence term\n",
    "    latent_loss=-0.5*torch.sum(1+z_logvar-z_mu**2-z_logvar.exp())\n",
    "    \n",
    "    return reconstruction_loss+latent_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer=optim.Adam(model.parameters(),lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================== Model Restored ==================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/pytorch/lib/python3.6/site-packages/torch/nn/modules/upsampling.py:135: UserWarning: nn.UpsamplingNearest2d is deprecated. Use nn.Upsample instead.\n",
      "  warnings.warn(\"nn.UpsamplingNearest2d is deprecated. Use nn.Upsample instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training Epoch: 0 \tn_iter: 0 \tLoss: 138415.562500]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nNAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x178522cc0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/pytorch/lib/python3.6/site-packages/torch/serialization.py:147: UserWarning: Couldn't retrieve source code for container of type VAE. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/anaconda/envs/pytorch/lib/python3.6/site-packages/torch/serialization.py:147: UserWarning: Couldn't retrieve source code for container of type Encoder. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/anaconda/envs/pytorch/lib/python3.6/site-packages/torch/serialization.py:147: UserWarning: Couldn't retrieve source code for container of type Decoder. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training Epoch: 0 \tn_iter: 50 \tLoss: 138660.515625]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x15aebfba8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-7:\n",
      "Process Process-8:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/anaconda/envs/pytorch/lib/python3.6/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/anaconda/envs/pytorch/lib/python3.6/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/anaconda/envs/pytorch/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/anaconda/envs/pytorch/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/anaconda/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 34, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/anaconda/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 34, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/anaconda/envs/pytorch/lib/python3.6/multiprocessing/queues.py\", line 341, in get\n",
      "    with self._rlock:\n",
      "  File \"/anaconda/envs/pytorch/lib/python3.6/multiprocessing/queues.py\", line 342, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/anaconda/envs/pytorch/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/anaconda/envs/pytorch/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/anaconda/envs/pytorch/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "KeyboardInterrupt\n",
      "  File \"/anaconda/envs/pytorch/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-f7a6a4bf5529>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mreconstructed_image\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloss_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreconstructed_image\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mz_mu\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mz_logvar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-24-7d95c532aaab>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;31m# decode the sampled z\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mz_mu\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mz_logvar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mz_mu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mz_mu\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-7c4e569b95d0>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m         \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/pytorch/lib/python3.6/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/pytorch/lib/python3.6/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    252\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m         return F.conv2d(input, self.weight, self.bias, self.stride,\n\u001b[0;32m--> 254\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/pytorch/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mconv2d\u001b[0;34m(input, weight, bias, stride, padding, dilation, groups)\u001b[0m\n\u001b[1;32m     50\u001b[0m     f = ConvNd(_pair(stride), _pair(padding), _pair(dilation), False,\n\u001b[1;32m     51\u001b[0m                _pair(0), groups, torch.backends.cudnn.benchmark, torch.backends.cudnn.enabled)\n\u001b[0;32m---> 52\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#writer=SummaryWriter()\n",
    "\n",
    "try:\n",
    "    model = torch.load('./model/dVAE.pkl')\n",
    "    print(\"\\n================== Model Restored ==================\\n\")\n",
    "except:\n",
    "    print(\"\\n============== No Model to be Restored ==============\\n\")\n",
    "    pass\n",
    "\n",
    "for i in range(num_epoch):\n",
    "    for batch_idx,[image,_] in enumerate(train_loader):\n",
    "        n_iter = (i*len(train_loader))+batch_idx\n",
    "        \n",
    "        if use_gpu:\n",
    "            x=Variable(image).cuda()\n",
    "        else:\n",
    "            x=Variable(image)\n",
    "        # x: batchsize x 3 x 32 x 32\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        reconstructed_image=model(x)\n",
    "        \n",
    "        loss=loss_func(reconstructed_image,x,model.z_mu,model.z_logvar)\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_idx%50==0:\n",
    "            print('[Training Epoch: {} \\tn_iter: {} \\tLoss: {:.6f}]'.format(\n",
    "                i, n_iter, loss.data[0]))\n",
    "            plt.imshow(reconstructed_image.cpu().data[0].numpy().transpose(1,2,0), cmap='hsv')\n",
    "            plt.show(block=True)\n",
    "            \n",
    "            \n",
    "        if batch_idx%200==0:\n",
    "            torch.save(model,'./model/dVAE_'+str(batch_idx)+'.pkl')\n",
    "            torch.save(model,'./model/dVAE.pkl')\n",
    "            \n",
    "#        writer.add_scalar('loss',loss.data[0] / len(x),n_iter)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.50167495,  0.51882547,  0.50488049],\n",
       "        [ 0.50362229,  0.52110022,  0.50821853],\n",
       "        [ 0.50677085,  0.52629131,  0.5122872 ],\n",
       "        ..., \n",
       "        [ 0.50487953,  0.52528012,  0.51094961],\n",
       "        [ 0.50053352,  0.51937765,  0.50787246],\n",
       "        [ 0.49801102,  0.51467299,  0.50054884]],\n",
       "\n",
       "       [[ 0.50242889,  0.5182777 ,  0.50406826],\n",
       "        [ 0.50056952,  0.51543373,  0.50166714],\n",
       "        [ 0.50297159,  0.52127045,  0.50581604],\n",
       "        ..., \n",
       "        [ 0.50049061,  0.51908678,  0.5037871 ],\n",
       "        [ 0.49627137,  0.51254183,  0.50101703],\n",
       "        [ 0.49686319,  0.51339757,  0.4998073 ]],\n",
       "\n",
       "       [[ 0.50129682,  0.51576805,  0.4953188 ],\n",
       "        [ 0.49827197,  0.51334023,  0.49408391],\n",
       "        [ 0.50127959,  0.51948518,  0.50065577],\n",
       "        ..., \n",
       "        [ 0.49968538,  0.51727796,  0.49878773],\n",
       "        [ 0.49528018,  0.51164144,  0.4942739 ],\n",
       "        [ 0.49547273,  0.51017439,  0.49070603]],\n",
       "\n",
       "       ..., \n",
       "       [[ 0.49100497,  0.49029741,  0.44566473],\n",
       "        [ 0.48601532,  0.48300046,  0.43550923],\n",
       "        [ 0.48218471,  0.4797906 ,  0.43315375],\n",
       "        ..., \n",
       "        [ 0.48125777,  0.47876942,  0.43277591],\n",
       "        [ 0.48366073,  0.48189175,  0.43622217],\n",
       "        [ 0.48620772,  0.48575422,  0.44301605]],\n",
       "\n",
       "       [[ 0.49512649,  0.49132121,  0.44560185],\n",
       "        [ 0.49338654,  0.4856714 ,  0.43743914],\n",
       "        [ 0.49074164,  0.48549119,  0.43835941],\n",
       "        ..., \n",
       "        [ 0.48900473,  0.4847222 ,  0.4365488 ],\n",
       "        [ 0.49134758,  0.48549041,  0.43792155],\n",
       "        [ 0.49094939,  0.48798376,  0.44383851]],\n",
       "\n",
       "       [[ 0.49508351,  0.49155441,  0.44895321],\n",
       "        [ 0.49760506,  0.49032691,  0.44502953],\n",
       "        [ 0.49761897,  0.49196446,  0.44702089],\n",
       "        ..., \n",
       "        [ 0.49587125,  0.49091923,  0.44527903],\n",
       "        [ 0.4950459 ,  0.48879603,  0.44441241],\n",
       "        [ 0.49147758,  0.48798656,  0.44683078]]], dtype=float32)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reconstructed_image.cpu().data[0].numpy().transpose(1,2,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "example_z = Variable(torch.FloatTensor(8*8*z_dim).normal_(0,1).view(64,-1))\n",
    "\n",
    "samples = model.decoder(example_z).data.numpy()\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "gs = gridspec.GridSpec(8, 8)\n",
    "gs.update(wspace=0.05, hspace=0.05)\n",
    "for i, sample in enumerate(samples):\n",
    "    ax = plt.subplot(gs[i])\n",
    "    plt.axis('off')\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_yticklabels([])\n",
    "    ax.set_aspect('equal')\n",
    "    plt.imshow(sample.reshape(32, 32,3).transpose(1,2,0), cmap='hsv')\n",
    "fig.savefig(\"generated_images_by_{}epochs.png\".format(num_epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3.6 [pytorch]",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
