{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  pytorch의 구현순서는 다음과 같다.\n",
    "1. Dataset, DataLoader 정의\n",
    "2. Model 정의\n",
    "3. Loss function 정의\n",
    "4. Back prop 정의\n",
    "5. Training \n",
    "6. Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## naive tutorial\n",
    "\n",
    "### 1. pre-requisite\n",
    "\n",
    "#### 1.1 install pytorch\n",
    "\n",
    "for windows users, you can use **[wsl(windows subsystem for Linux)](https://msdn.microsoft.com/en-us/commandline/wsl/install_guide)** <br>\n",
    "\n",
    "[GPU support is now available too for wsl](https://datascience.stackexchange.com/questions/17776/how-to-install-pytorch-in-windows) <br>\n",
    "\n",
    "requirements : <br>\n",
    "* [conda & pytorch](http://pytorch.org/)\n",
    "\n",
    "#### 1.2 install tensorboard\n",
    "```bash\n",
    "pip install tensorboard-pytorch \n",
    "pip install tensorflow-tensorboard\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd.variable import Variable\n",
    "import torchvision.utils as vutils\n",
    "import numpy as np\n",
    "import torchvision.models as models\n",
    "from torchvision import datasets\n",
    "from tensorboard import SummaryWriter\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dataset, DataLoader 정의\n",
    "\n",
    "먼저 customized Dataset으로 진행할 때의 방식은 다음과 같다 : <br>\n",
    "```python\n",
    "\n",
    "des_dir = \"./somewhere\"\n",
    "\n",
    "dataset = dset.ImageFolder(root=des_dir,\n",
    "                           transform=transforms.Compose([\n",
    "                               transforms.Scale(imageSize),\n",
    "                               transforms.ToTensor(),\n",
    "                               transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "                           ]))\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(dataset,\n",
    "                                         batch_size= batchSize,\n",
    "                                         shuffle=True)\n",
    "\n",
    "```\n",
    "\n",
    "**DATASET**은 파일을 파이썬에 불러오는데 그 역할이 있다. <br>\n",
    "따라서 \n",
    "```python\n",
    "__getitem__\n",
    "```\n",
    "을 정의할 필요가 있는데, sample code는 본인의 [deep speech](https://github.com/YBIGTA/Deep_learning/blob/master/RNN/deep_speech/implementation/deep%20speech1%20%EA%B5%AC%ED%98%84.ipynb)를 참고한다. <br>\n",
    "\n",
    "**DATALOADER**는 dataset을 받아 slicing을 해주는데에 그 의미가 있다. <br>\n",
    "이는 pytorch의 \n",
    "```python\n",
    "self.collate_fn \n",
    "``` \n",
    "을 구현하여, 각 크기가 다른, 혹은 형태가 다른 아이템을을 어떻게 같이 slicing을 할 것인가를 정의해야 한다.<br>\n",
    "```python\n",
    "self.collate_fn \n",
    "``` \n",
    "에는 slicing된 각 dataset의 데이터가 list형태로 들어가고, 이 output을 정의해야한다. <br>\n",
    "\n",
    "예를들어 음성은 각 사이즈가 다 다른데, 어떻게 collation을 할 지를 정의해줘야 하는 것이다.<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# dataset & dataLoader for Mnists.\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=True, download=True,\n",
    "                   transform=transforms.ToTensor()),\n",
    "    batch_size=100, shuffle=True,)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=False, transform=transforms.ToTensor()),\n",
    "    batch_size=100, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. modeling\n",
    "modeling은 **nn.module**을 상속받아 Class로 바인딩한다. <br>\n",
    "\n",
    "back propagation은 정의할 필요가 없고,<br>\n",
    "\n",
    "1. __init__을 통하여 Model에 어떤 레이어들이 있는지 정의한 후,\n",
    "2. forward()를 통하여 데이터가 각 레이어에 어떻게 들어가게되는지 정의한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# modeling\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(784, 400)\n",
    "        self.fc21 = nn.Linear(400, 20)\n",
    "        self.fc22 = nn.Linear(400, 20)\n",
    "        self.fc3 = nn.Linear(20, 400)\n",
    "        self.fc4 = nn.Linear(400, 784)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def encode(self, x):\n",
    "        h1 = self.relu(self.fc1(x))\n",
    "        return self.fc21(h1), self.fc22(h1)\n",
    "\n",
    "    def reparametrize(self, mu, logvar):\n",
    "        std = logvar.mul(0.5).exp_()\n",
    "        if 0:\n",
    "            eps = torch.cuda.FloatTensor(std.size()).normal_()\n",
    "        else:\n",
    "            eps = torch.FloatTensor(std.size()).normal_()\n",
    "        eps = Variable(eps)\n",
    "        return eps.mul(std).add_(mu)\n",
    "\n",
    "    def decode(self, z):\n",
    "        h3 = self.relu(self.fc3(z))\n",
    "        return self.sigmoid(self.fc4(h3))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x.view(-1, 784))\n",
    "        z = self.reparametrize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 모델 초기화\n",
    "model = VAE()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Loss function 구현\n",
    "\n",
    "loss function을 정의하여, model에서 나온 output을 넣어 역전파를 구할 장소를 만든다. <br>\n",
    "\n",
    "함수로서 정의하면 되고, 일반적인 경우에선 그냥 NLL등을 사용하면 된다.<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reconstruction_function = nn.BCELoss()\n",
    "reconstruction_function.size_average = False\n",
    "\n",
    "\n",
    "def loss_function(recon_x, x, mu, logvar):\n",
    "    BCE = reconstruction_function(recon_x, x)\n",
    "\n",
    "    # see Appendix B from VAE paper:\n",
    "    # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n",
    "    # https://arxiv.org/abs/1312.6114\n",
    "    # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    KLD_element = mu.pow(2).add_(logvar.exp()).mul_(-1).add_(1).add_(logvar)\n",
    "    KLD = torch.sum(KLD_element).mul_(-0.5)\n",
    "\n",
    "    return BCE + KLD\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Back prop method 구현.\n",
    "\n",
    "굳이 따로 때넨 이유는, 코드가 때에 따라 길어질 여지가 있고 하나의 명백한 학문이기 때문이다.\n",
    "\n",
    "모델의 인자(트레이닝을 할 변수들)를 당연하게도 받는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training, implementing tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 122.067793\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "Train Epoch: 1 [10000/60000 (17%)]\tLoss: 119.392334\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "Train Epoch: 1 [20000/60000 (33%)]\tLoss: 125.530742\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "Train Epoch: 1 [30000/60000 (50%)]\tLoss: 124.549688\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "Train Epoch: 1 [40000/60000 (67%)]\tLoss: 117.644824\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "Train Epoch: 1 [50000/60000 (83%)]\tLoss: 118.950420\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "# tensorboard에 필요한 것. \n",
    "writer = SummaryWriter()\n",
    "#epoch 수\n",
    "epoch = 1\n",
    "# 몇 번째마다 로깅 할 것인가.\n",
    "embedding_log = 5\n",
    "\n",
    "#train\n",
    "for i in range(epoch):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, (data, label) in enumerate(train_loader):\n",
    "        n_iter = (i*len(train_loader))+batch_idx\n",
    "        print(len(label))\n",
    "        data = Variable(data)\n",
    "        if 0:\n",
    "            data = data.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, logvar = model(data)\n",
    "        loss = loss_function(recon_batch, data, mu, logvar)\n",
    "        loss.backward()\n",
    "        train_loss += loss.data[0]\n",
    "        optimizer.step()\n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader),\n",
    "                loss.data[0] / len(data)))\n",
    "            \n",
    "        writer.add_scalar('loss',loss.data[0] / len(data),n_iter)\n",
    "        for tag, value in model.named_parameters():\n",
    "            writer.add_histogram(tag, value.data.numpy(), global_step = n_iter)\n",
    "        if batch_idx % embedding_log == 0:\n",
    "            #we need  dimension for tensor to visualize it\n",
    "            writer.add_embedding(recon_batch.data,metadata = label, label_img = data.data,global_step = n_iter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.00205815, -0.00553272,  0.01402932, ...,  0.00380697,\n",
       "        -0.03047704,  0.00743232],\n",
       "       [-0.00973061,  0.02102149, -0.00396892, ...,  0.0184958 ,\n",
       "         0.01318543,  0.00028399],\n",
       "       [-0.02935295, -0.02123811, -0.01930187, ..., -0.02751336,\n",
       "        -0.01735669, -0.00705339],\n",
       "       ..., \n",
       "       [-0.02532171,  0.00659803,  0.0349379 , ..., -0.01281032,\n",
       "         0.01002253,  0.02722737],\n",
       "       [-0.01069853, -0.01685899,  0.0151909 , ...,  0.01378453,\n",
       "        -0.02534758,  0.0127874 ],\n",
       "       [-0.02630001, -0.02807823,  0.02950477, ...,  0.00298915,\n",
       "         0.02662011, -0.00847801]], dtype=float32)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value.data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "writer.add_histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fc4.bias'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "206.529765625"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss.data[0] / len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21168.311"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss.data.numpy()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.FloatTensor"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(recon_batch.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fc1.weight\n",
      "Parameter containing:\n",
      "-2.0581e-03 -5.5327e-03  1.4029e-02  ...   3.8070e-03 -3.0477e-02  7.4323e-03\n",
      "-9.7306e-03  2.1021e-02 -3.9689e-03  ...   1.8496e-02  1.3185e-02  2.8399e-04\n",
      "-2.9353e-02 -2.1238e-02 -1.9302e-02  ...  -2.7513e-02 -1.7357e-02 -7.0534e-03\n",
      "                ...                   ⋱                   ...                \n",
      "-2.5322e-02  6.5980e-03  3.4938e-02  ...  -1.2810e-02  1.0023e-02  2.7227e-02\n",
      "-1.0699e-02 -1.6859e-02  1.5191e-02  ...   1.3785e-02 -2.5348e-02  1.2787e-02\n",
      "-2.6300e-02 -2.8078e-02  2.9505e-02  ...   2.9891e-03  2.6620e-02 -8.4780e-03\n",
      "[torch.FloatTensor of size 400x784]\n",
      "\n",
      "fc1.bias\n",
      "Parameter containing:\n",
      " 0.0064\n",
      " 0.0191\n",
      "-0.0027\n",
      " 0.0070\n",
      "-0.0627\n",
      " 0.0787\n",
      " 0.0148\n",
      " 0.0199\n",
      " 0.0165\n",
      " 0.0662\n",
      "-0.0344\n",
      " 0.1589\n",
      " 0.0203\n",
      " 0.0701\n",
      " 0.0223\n",
      "-0.1591\n",
      " 0.1117\n",
      "-0.0385\n",
      " 0.1145\n",
      " 0.1398\n",
      " 0.0073\n",
      "-0.0171\n",
      " 0.1323\n",
      "-0.0441\n",
      "-0.0026\n",
      "-0.0834\n",
      "-0.0199\n",
      "-0.0142\n",
      " 0.0840\n",
      " 0.0750\n",
      " 0.1920\n",
      " 0.1390\n",
      " 0.0182\n",
      "-0.0372\n",
      " 0.0213\n",
      "-0.0453\n",
      "-0.0227\n",
      "-0.0008\n",
      " 0.1093\n",
      "-0.0221\n",
      " 0.0077\n",
      " 0.0153\n",
      " 0.1274\n",
      "-0.0076\n",
      " 0.0549\n",
      "-0.0042\n",
      " 0.0047\n",
      " 0.0595\n",
      " 0.2045\n",
      " 0.1215\n",
      " 0.0578\n",
      " 0.0921\n",
      "-0.0065\n",
      " 0.0191\n",
      " 0.0195\n",
      " 0.0252\n",
      " 0.1551\n",
      " 0.0819\n",
      " 0.1321\n",
      " 0.1098\n",
      " 0.1148\n",
      "-0.0186\n",
      "-0.0425\n",
      "-0.0071\n",
      " 0.1388\n",
      "-0.0348\n",
      "-0.0052\n",
      " 0.1580\n",
      " 0.1891\n",
      " 0.2264\n",
      " 0.0416\n",
      "-0.0882\n",
      " 0.0034\n",
      " 0.2055\n",
      " 0.0940\n",
      " 0.0562\n",
      " 0.3147\n",
      " 0.0811\n",
      " 0.1499\n",
      " 0.3959\n",
      "-0.0468\n",
      "-0.0296\n",
      " 0.0510\n",
      "-0.0458\n",
      " 0.1195\n",
      " 0.0636\n",
      "-0.0120\n",
      "-0.0100\n",
      " 0.0302\n",
      " 0.0480\n",
      " 0.1354\n",
      " 0.0121\n",
      "-0.0037\n",
      " 0.0131\n",
      " 0.1824\n",
      "-0.0003\n",
      " 0.0378\n",
      "-0.0815\n",
      "-0.0527\n",
      " 0.0085\n",
      " 0.1843\n",
      "-0.0062\n",
      " 0.2263\n",
      "-0.0174\n",
      " 0.0951\n",
      " 0.0006\n",
      " 0.0812\n",
      "-0.0151\n",
      " 0.2740\n",
      " 0.0131\n",
      " 0.2702\n",
      " 0.0016\n",
      " 0.0919\n",
      " 0.0609\n",
      "-0.0904\n",
      "-0.0063\n",
      "-0.0355\n",
      "-0.0717\n",
      " 0.1301\n",
      "-0.0188\n",
      "-0.0068\n",
      " 0.1412\n",
      " 0.1983\n",
      " 0.0811\n",
      " 0.0294\n",
      "-0.1034\n",
      " 0.0816\n",
      "-0.0046\n",
      "-0.0273\n",
      "-0.0180\n",
      " 0.1080\n",
      " 0.2903\n",
      " 0.2258\n",
      "-0.0237\n",
      "-0.0854\n",
      " 0.0821\n",
      "-0.0796\n",
      "-0.0064\n",
      " 0.0157\n",
      "-0.0066\n",
      " 0.1598\n",
      "-0.0183\n",
      " 0.2239\n",
      " 0.2099\n",
      "-0.0904\n",
      " 0.1719\n",
      " 0.0202\n",
      " 0.0396\n",
      "-0.0216\n",
      " 0.1052\n",
      " 0.0086\n",
      "-0.0110\n",
      " 0.1537\n",
      "-0.0245\n",
      " 0.0033\n",
      "-0.0300\n",
      "-0.0862\n",
      "-0.0189\n",
      " 0.0979\n",
      "-0.0996\n",
      " 0.1003\n",
      " 0.2492\n",
      " 0.0070\n",
      "-0.0264\n",
      " 0.1073\n",
      " 0.1521\n",
      " 0.1293\n",
      " 0.0793\n",
      " 0.1281\n",
      " 0.1127\n",
      " 0.0571\n",
      " 0.0827\n",
      " 0.1171\n",
      " 0.2920\n",
      " 0.1529\n",
      " 0.0581\n",
      " 0.0090\n",
      " 0.1563\n",
      " 0.1431\n",
      " 0.0210\n",
      " 0.0757\n",
      " 0.2070\n",
      "-0.0105\n",
      "-0.0312\n",
      " 0.2912\n",
      "-0.0488\n",
      " 0.1708\n",
      "-0.0415\n",
      " 0.1721\n",
      " 0.0063\n",
      "-0.0434\n",
      "-0.0195\n",
      " 0.2187\n",
      "-0.0666\n",
      " 0.0209\n",
      " 0.2160\n",
      "-0.0173\n",
      "-0.1966\n",
      "-0.1167\n",
      "-0.0193\n",
      " 0.1938\n",
      " 0.1407\n",
      " 0.1464\n",
      " 0.1755\n",
      "-0.0431\n",
      " 0.0922\n",
      "-0.1024\n",
      "-0.0668\n",
      "-0.0031\n",
      " 0.0376\n",
      "-0.0530\n",
      " 0.0207\n",
      " 0.2668\n",
      "-0.0120\n",
      " 0.0969\n",
      " 0.0166\n",
      " 0.0644\n",
      " 0.0220\n",
      "-0.0428\n",
      " 0.1491\n",
      "-0.0558\n",
      " 0.0103\n",
      "-0.0128\n",
      "-0.0555\n",
      " 0.0195\n",
      " 0.0244\n",
      " 0.0248\n",
      " 0.1217\n",
      " 0.2782\n",
      " 0.3169\n",
      "-0.0197\n",
      " 0.0520\n",
      " 0.0854\n",
      " 0.0185\n",
      "-0.0432\n",
      "-0.1958\n",
      " 0.0646\n",
      " 0.0940\n",
      "-0.0159\n",
      "-0.0213\n",
      "-0.0079\n",
      " 0.1022\n",
      " 0.0096\n",
      "-0.0010\n",
      "-0.1037\n",
      "-0.0493\n",
      " 0.2669\n",
      " 0.2107\n",
      " 0.0219\n",
      " 0.0879\n",
      " 0.1872\n",
      " 0.0110\n",
      " 0.1531\n",
      "-0.0149\n",
      " 0.1501\n",
      " 0.1135\n",
      "-0.0218\n",
      "-0.0410\n",
      "-0.0339\n",
      "-0.0103\n",
      "-0.0178\n",
      "-0.0306\n",
      "-0.0257\n",
      " 0.0194\n",
      "-0.0190\n",
      "-0.0118\n",
      " 0.0112\n",
      " 0.1492\n",
      "-0.0544\n",
      " 0.0784\n",
      "-0.0375\n",
      " 0.0463\n",
      " 0.0180\n",
      " 0.1977\n",
      " 0.1308\n",
      " 0.0067\n",
      " 0.1190\n",
      "-0.0012\n",
      "-0.0355\n",
      " 0.2004\n",
      " 0.1701\n",
      " 0.1954\n",
      " 0.2071\n",
      "-0.0206\n",
      "-0.0303\n",
      " 0.0184\n",
      " 0.1217\n",
      " 0.0190\n",
      " 0.0036\n",
      "-0.0333\n",
      "-0.0277\n",
      " 0.0137\n",
      " 0.0844\n",
      "-0.0241\n",
      "-0.0275\n",
      "-0.0063\n",
      " 0.2609\n",
      "-0.0403\n",
      " 0.0556\n",
      "-0.0152\n",
      " 0.0533\n",
      "-0.0114\n",
      " 0.0389\n",
      " 0.0142\n",
      " 0.1424\n",
      "-0.0788\n",
      "-0.0347\n",
      "-0.0421\n",
      "-0.0028\n",
      " 0.1830\n",
      "-0.0342\n",
      "-0.0153\n",
      " 0.1455\n",
      " 0.0498\n",
      "-0.0110\n",
      " 0.1867\n",
      " 0.0992\n",
      " 0.0094\n",
      "-0.0323\n",
      "-0.0010\n",
      " 0.1323\n",
      " 0.0153\n",
      "-0.0149\n",
      " 0.1106\n",
      " 0.0584\n",
      "-0.0084\n",
      "-0.0301\n",
      " 0.3445\n",
      " 0.1031\n",
      "-0.0568\n",
      " 0.0063\n",
      "-0.0444\n",
      "-0.0312\n",
      " 0.0408\n",
      " 0.1665\n",
      "-0.0029\n",
      " 0.0184\n",
      " 0.1401\n",
      " 0.3389\n",
      " 0.0768\n",
      "-0.0104\n",
      "-0.0330\n",
      " 0.2162\n",
      " 0.0337\n",
      " 0.1859\n",
      "-0.0299\n",
      " 0.0449\n",
      " 0.1841\n",
      "-0.0783\n",
      " 0.2774\n",
      "-0.0353\n",
      " 0.2465\n",
      " 0.0787\n",
      " 0.2273\n",
      " 0.0129\n",
      "-0.0841\n",
      "-0.0367\n",
      "-0.0424\n",
      "-0.0183\n",
      " 0.0064\n",
      " 0.1879\n",
      " 0.0572\n",
      "-0.0406\n",
      " 0.0196\n",
      "-0.0067\n",
      "-0.0159\n",
      "-0.0130\n",
      "-0.0131\n",
      "-0.0247\n",
      "-0.0058\n",
      " 0.0782\n",
      " 0.0189\n",
      " 0.1543\n",
      " 0.2228\n",
      " 0.0405\n",
      " 0.0710\n",
      " 0.1536\n",
      "-0.0103\n",
      "-0.0060\n",
      " 0.0074\n",
      " 0.0120\n",
      " 0.0533\n",
      " 0.0192\n",
      "-0.0136\n",
      "-0.0029\n",
      " 0.1409\n",
      "-0.0328\n",
      "-0.0446\n",
      " 0.0149\n",
      " 0.1774\n",
      " 0.0168\n",
      " 0.0175\n",
      " 0.0051\n",
      " 0.1034\n",
      "-0.0235\n",
      " 0.0790\n",
      "-0.0135\n",
      " 0.0284\n",
      " 0.0754\n",
      "-0.0323\n",
      "[torch.FloatTensor of size 400]\n",
      "\n",
      "fc21.weight\n",
      "Parameter containing:\n",
      "-0.0249  0.0713 -0.0630  ...   0.0622  0.0426  0.0367\n",
      "-0.0501 -0.0345  0.0638  ...  -0.0417  0.0083  0.0328\n",
      "-0.0115  0.0245  0.0477  ...   0.0073  0.0569  0.0334\n",
      "          ...             ⋱             ...          \n",
      " 0.0238 -0.0005 -0.0473  ...  -0.0712  0.0206 -0.0271\n",
      "-0.0272  0.0342  0.0675  ...  -0.0052  0.0436 -0.0407\n",
      "-0.0332 -0.0560 -0.0428  ...   0.0166 -0.0043  0.0095\n",
      "[torch.FloatTensor of size 20x400]\n",
      "\n",
      "fc21.bias\n",
      "Parameter containing:\n",
      " 0.0491\n",
      " 0.0401\n",
      "-0.1102\n",
      " 0.1557\n",
      " 0.0494\n",
      " 0.0369\n",
      "-0.0477\n",
      "-0.0435\n",
      " 0.0086\n",
      "-0.1002\n",
      " 0.1292\n",
      " 0.1035\n",
      " 0.0179\n",
      " 0.0289\n",
      "-0.0907\n",
      " 0.1270\n",
      " 0.0328\n",
      "-0.0837\n",
      " 0.0910\n",
      " 0.1166\n",
      "[torch.FloatTensor of size 20]\n",
      "\n",
      "fc22.weight\n",
      "Parameter containing:\n",
      " 0.0500 -0.0343 -0.0467  ...  -0.0105 -0.0252  0.0390\n",
      "-0.0314  0.0313 -0.0141  ...  -0.0235 -0.0586  0.0106\n",
      "-0.0393  0.0298  0.0331  ...   0.0252 -0.0337 -0.0420\n",
      "          ...             ⋱             ...          \n",
      " 0.0502  0.0176  0.0342  ...  -0.0361 -0.0288  0.0006\n",
      "-0.0248  0.0303 -0.0009  ...  -0.0312 -0.0659 -0.0025\n",
      " 0.0201 -0.0220 -0.0459  ...  -0.0097 -0.0554  0.0149\n",
      "[torch.FloatTensor of size 20x400]\n",
      "\n",
      "fc22.bias\n",
      "Parameter containing:\n",
      "-0.0882\n",
      "-0.0839\n",
      "-0.1010\n",
      "-0.0994\n",
      "-0.0689\n",
      " 0.0220\n",
      "-0.0385\n",
      "-0.0419\n",
      " 0.0559\n",
      "-0.0322\n",
      " 0.0272\n",
      " 0.0081\n",
      " 0.0213\n",
      "-0.0367\n",
      "-0.0396\n",
      "-0.0267\n",
      "-0.0379\n",
      "-0.0170\n",
      "-0.0005\n",
      "-0.0432\n",
      "[torch.FloatTensor of size 20]\n",
      "\n",
      "fc3.weight\n",
      "Parameter containing:\n",
      "-0.0820 -0.1344 -0.2666  ...  -0.1611 -0.2477  0.0941\n",
      "-0.3051 -0.1937 -0.1572  ...  -0.3437  0.1772  0.1647\n",
      "-0.1889  0.2378  0.4875  ...  -0.1377 -0.2785  0.0116\n",
      "          ...             ⋱             ...          \n",
      " 0.3179  0.3072  0.2736  ...   0.1367 -0.0169  0.1754\n",
      " 0.3092 -0.2894  0.3125  ...  -0.2913  0.2759 -0.0805\n",
      " 0.0196 -0.0923 -0.3759  ...  -0.1696 -0.1805  0.2272\n",
      "[torch.FloatTensor of size 400x20]\n",
      "\n",
      "fc3.bias\n",
      "Parameter containing:\n",
      " 0.1259\n",
      "-0.0046\n",
      " 0.2589\n",
      " 0.4901\n",
      " 0.1913\n",
      " 0.1303\n",
      " 0.0728\n",
      "-0.0691\n",
      " 0.0482\n",
      " 0.0870\n",
      " 0.1936\n",
      " 0.1843\n",
      " 0.2791\n",
      " 0.1591\n",
      "-0.0502\n",
      " 0.1724\n",
      " 0.3160\n",
      " 0.3932\n",
      " 0.2537\n",
      " 0.2570\n",
      " 0.1132\n",
      " 0.4090\n",
      "-0.0642\n",
      " 0.0793\n",
      " 0.1331\n",
      " 0.3328\n",
      " 0.0411\n",
      " 0.1276\n",
      "-0.0541\n",
      " 0.0976\n",
      " 0.0356\n",
      " 0.3278\n",
      " 0.2189\n",
      " 0.0675\n",
      " 0.5143\n",
      " 0.2838\n",
      "-0.1195\n",
      " 0.2890\n",
      " 0.1188\n",
      " 0.3288\n",
      "-0.0903\n",
      " 0.3392\n",
      " 0.0168\n",
      " 0.1928\n",
      "-0.0579\n",
      " 0.2727\n",
      " 0.2729\n",
      " 0.2349\n",
      " 0.0317\n",
      "-0.1212\n",
      " 0.1390\n",
      " 0.0847\n",
      " 0.1241\n",
      " 0.0445\n",
      " 0.4113\n",
      " 0.3756\n",
      " 0.0361\n",
      " 0.0255\n",
      " 0.1580\n",
      " 0.3596\n",
      " 0.2359\n",
      " 0.1683\n",
      " 0.1417\n",
      " 0.1032\n",
      " 0.2514\n",
      "-0.0693\n",
      " 0.1966\n",
      " 0.0879\n",
      "-0.0294\n",
      " 0.2902\n",
      " 0.2208\n",
      " 0.1251\n",
      " 0.2243\n",
      " 0.0416\n",
      " 0.0301\n",
      " 0.3001\n",
      " 0.3224\n",
      " 0.2986\n",
      " 0.0996\n",
      " 0.1816\n",
      " 0.2675\n",
      " 0.2478\n",
      " 0.1030\n",
      " 0.1056\n",
      " 0.2769\n",
      " 0.3632\n",
      " 0.0738\n",
      " 0.1984\n",
      " 0.2251\n",
      "-0.0456\n",
      " 0.2247\n",
      " 0.0666\n",
      " 0.0292\n",
      " 0.4008\n",
      " 0.0483\n",
      "-0.1004\n",
      " 0.3394\n",
      "-0.0131\n",
      " 0.1149\n",
      " 0.1761\n",
      " 0.0597\n",
      " 0.1242\n",
      " 0.0133\n",
      "-0.0668\n",
      " 0.0121\n",
      " 0.3990\n",
      "-0.0689\n",
      " 0.2518\n",
      " 0.1702\n",
      " 0.4347\n",
      " 0.0338\n",
      " 0.2759\n",
      " 0.2097\n",
      " 0.2619\n",
      " 0.1909\n",
      " 0.2529\n",
      " 0.1196\n",
      " 0.2862\n",
      " 0.1063\n",
      "-0.0370\n",
      " 0.3684\n",
      " 0.0320\n",
      " 0.3002\n",
      " 0.0001\n",
      "-0.0623\n",
      " 0.0404\n",
      " 0.2710\n",
      " 0.3360\n",
      " 0.1702\n",
      " 0.2603\n",
      "-0.0790\n",
      " 0.3824\n",
      " 0.0802\n",
      " 0.2865\n",
      " 0.2679\n",
      " 0.1863\n",
      " 0.4764\n",
      " 0.1499\n",
      "-0.0003\n",
      " 0.2848\n",
      " 0.3457\n",
      " 0.1554\n",
      "-0.0005\n",
      "-0.0625\n",
      "-0.0313\n",
      " 0.1504\n",
      " 0.3695\n",
      " 0.1479\n",
      " 0.1517\n",
      " 0.0016\n",
      " 0.1694\n",
      " 0.2037\n",
      " 0.1915\n",
      " 0.0526\n",
      " 0.2943\n",
      " 0.2389\n",
      " 0.0279\n",
      " 0.0823\n",
      " 0.3218\n",
      " 0.0867\n",
      "-0.0389\n",
      " 0.1320\n",
      " 0.4117\n",
      "-0.0264\n",
      " 0.1415\n",
      " 0.2451\n",
      " 0.2255\n",
      " 0.1343\n",
      " 0.1748\n",
      "-0.0821\n",
      " 0.3239\n",
      " 0.2515\n",
      " 0.2405\n",
      " 0.2403\n",
      "-0.0002\n",
      " 0.3263\n",
      " 0.0326\n",
      " 0.1217\n",
      " 0.2377\n",
      " 0.0518\n",
      "-0.0644\n",
      " 0.0243\n",
      " 0.3278\n",
      "-0.0030\n",
      " 0.3151\n",
      " 0.1798\n",
      "-0.1414\n",
      " 0.0241\n",
      " 0.3665\n",
      "-0.0357\n",
      "-0.0941\n",
      " 0.2091\n",
      " 0.4767\n",
      " 0.3255\n",
      " 0.0911\n",
      " 0.0609\n",
      " 0.1013\n",
      " 0.3683\n",
      " 0.0020\n",
      " 0.1499\n",
      " 0.0443\n",
      " 0.0276\n",
      " 0.1511\n",
      " 0.1156\n",
      " 0.1787\n",
      " 0.2654\n",
      " 0.1820\n",
      " 0.1049\n",
      "-0.0889\n",
      " 0.4952\n",
      " 0.3640\n",
      " 0.0830\n",
      "-0.0264\n",
      " 0.1406\n",
      " 0.3936\n",
      "-0.1034\n",
      " 0.1010\n",
      "-0.0148\n",
      "-0.0128\n",
      " 0.0844\n",
      " 0.0681\n",
      " 0.1517\n",
      " 0.4527\n",
      " 0.0321\n",
      " 0.3814\n",
      " 0.1614\n",
      " 0.1846\n",
      " 0.2438\n",
      " 0.3134\n",
      " 0.1355\n",
      " 0.0919\n",
      " 0.2531\n",
      " 0.3179\n",
      " 0.2946\n",
      " 0.0996\n",
      " 0.3578\n",
      "-0.0571\n",
      " 0.2907\n",
      " 0.1927\n",
      " 0.3833\n",
      " 0.2745\n",
      " 0.1051\n",
      " 0.1576\n",
      " 0.0864\n",
      " 0.2611\n",
      " 0.3344\n",
      " 0.3176\n",
      " 0.1626\n",
      " 0.0247\n",
      " 0.0423\n",
      " 0.3567\n",
      " 0.3740\n",
      " 0.0294\n",
      "-0.1024\n",
      " 0.3760\n",
      " 0.5473\n",
      " 0.0011\n",
      " 0.3950\n",
      "-0.0579\n",
      " 0.1513\n",
      " 0.2776\n",
      " 0.1108\n",
      " 0.0270\n",
      " 0.2233\n",
      " 0.3295\n",
      " 0.1771\n",
      "-0.1145\n",
      " 0.2051\n",
      " 0.2625\n",
      " 0.2048\n",
      " 0.4454\n",
      " 0.1633\n",
      "-0.0861\n",
      " 0.0853\n",
      "-0.0540\n",
      " 0.1637\n",
      " 0.1547\n",
      " 0.2513\n",
      " 0.4387\n",
      " 0.1749\n",
      " 0.2269\n",
      " 0.0376\n",
      " 0.1859\n",
      " 0.0548\n",
      " 0.2500\n",
      " 0.1900\n",
      " 0.3009\n",
      " 0.3136\n",
      " 0.0134\n",
      "-0.0238\n",
      " 0.0581\n",
      " 0.1654\n",
      " 0.2857\n",
      " 0.1599\n",
      " 0.0401\n",
      " 0.2295\n",
      " 0.2323\n",
      " 0.1790\n",
      "-0.0028\n",
      " 0.1571\n",
      " 0.0214\n",
      "-0.0430\n",
      " 0.2697\n",
      " 0.0898\n",
      " 0.3920\n",
      " 0.0948\n",
      " 0.3431\n",
      " 0.3476\n",
      " 0.0225\n",
      " 0.1312\n",
      " 0.2075\n",
      " 0.5371\n",
      " 0.2890\n",
      " 0.1140\n",
      " 0.2735\n",
      " 0.1543\n",
      " 0.2505\n",
      " 0.0861\n",
      " 0.2709\n",
      " 0.0061\n",
      " 0.0919\n",
      " 0.1262\n",
      " 0.0557\n",
      " 0.1116\n",
      " 0.1896\n",
      "-0.1073\n",
      "-0.0984\n",
      " 0.1219\n",
      " 0.2249\n",
      " 0.3901\n",
      " 0.1116\n",
      " 0.0002\n",
      "-0.1133\n",
      " 0.0502\n",
      "-0.0103\n",
      " 0.1041\n",
      " 0.1802\n",
      "-0.0267\n",
      " 0.2889\n",
      " 0.1088\n",
      " 0.3110\n",
      "-0.0487\n",
      " 0.3705\n",
      " 0.1531\n",
      " 0.0225\n",
      "-0.0618\n",
      " 0.2823\n",
      " 0.1207\n",
      " 0.3475\n",
      " 0.0491\n",
      " 0.1667\n",
      " 0.2997\n",
      " 0.3713\n",
      " 0.2652\n",
      " 0.2111\n",
      " 0.3309\n",
      " 0.3124\n",
      " 0.1263\n",
      " 0.3167\n",
      " 0.2836\n",
      " 0.3344\n",
      " 0.3421\n",
      " 0.3511\n",
      " 0.2053\n",
      "-0.0121\n",
      " 0.0323\n",
      " 0.2784\n",
      " 0.1619\n",
      " 0.1419\n",
      "-0.0552\n",
      "-0.0069\n",
      " 0.1489\n",
      " 0.3013\n",
      "-0.0241\n",
      "-0.0332\n",
      "-0.0592\n",
      " 0.2074\n",
      "-0.0421\n",
      " 0.2980\n",
      "-0.1337\n",
      " 0.2827\n",
      " 0.2459\n",
      " 0.0056\n",
      "-0.0173\n",
      " 0.4068\n",
      " 0.0351\n",
      " 0.2703\n",
      " 0.0395\n",
      " 0.3999\n",
      " 0.0590\n",
      " 0.0265\n",
      " 0.2126\n",
      " 0.1522\n",
      " 0.0647\n",
      " 0.0557\n",
      "-0.0159\n",
      " 0.2090\n",
      "-0.1427\n",
      " 0.0814\n",
      " 0.0134\n",
      "[torch.FloatTensor of size 400]\n",
      "\n",
      "fc4.weight\n",
      "Parameter containing:\n",
      "-2.0761e-02 -1.2544e-02 -2.3943e-02  ...  -6.3392e-02 -8.3353e-02 -3.7018e-02\n",
      "-5.8192e-02 -2.7336e-02 -9.3054e-02  ...  -4.8159e-02 -1.3472e-02 -5.9675e-02\n",
      "-8.3438e-02 -4.8910e-02 -5.4601e-02  ...  -1.9639e-02 -7.8378e-02 -3.2633e-02\n",
      "                ...                   ⋱                   ...                \n",
      "-4.9463e-02 -6.3197e-02 -5.2436e-02  ...  -1.0771e-03 -5.6746e-02 -2.1574e-02\n",
      "-1.4586e-02  2.0309e-03 -8.9573e-02  ...  -9.0493e-02 -3.6308e-02 -3.3562e-02\n",
      " 3.2467e-03 -3.9525e-03 -8.6738e-02  ...   5.6607e-03 -3.5696e-02 -5.7859e-02\n",
      "[torch.FloatTensor of size 784x400]\n",
      "\n",
      "fc4.bias\n",
      "Parameter containing:\n",
      "-2.5183e-02\n",
      "-8.0535e-02\n",
      "-7.3318e-02\n",
      "-5.3777e-02\n",
      "-2.3824e-02\n",
      "-6.5854e-03\n",
      "-3.8002e-02\n",
      "-5.6182e-02\n",
      "-5.1680e-02\n",
      "-3.9077e-02\n",
      " 9.4992e-03\n",
      "-2.3393e-02\n",
      " 1.2723e-02\n",
      "-3.3842e-02\n",
      " 9.0467e-03\n",
      "-1.4607e-02\n",
      " 4.5091e-03\n",
      " 1.3198e-02\n",
      "-6.9183e-02\n",
      "-6.7946e-02\n",
      "-4.2287e-02\n",
      "-1.0193e-02\n",
      "-6.2135e-02\n",
      "-3.7348e-02\n",
      "-1.8598e-02\n",
      "-8.1430e-02\n",
      "-2.9743e-02\n",
      "-3.7722e-02\n",
      "-3.5468e-02\n",
      "-4.6177e-02\n",
      "-4.4245e-02\n",
      "-6.3582e-02\n",
      "-6.5001e-02\n",
      "-3.4812e-02\n",
      " 4.4389e-03\n",
      "-3.8160e-02\n",
      "-3.7000e-03\n",
      "-7.8410e-02\n",
      "-6.8678e-02\n",
      "-5.0993e-03\n",
      "-4.4369e-02\n",
      "-7.4247e-02\n",
      "-6.3556e-02\n",
      "-1.0688e-02\n",
      "-2.7801e-02\n",
      "-1.9221e-02\n",
      "-3.6868e-03\n",
      "-3.0241e-03\n",
      "-3.2119e-02\n",
      "-6.6071e-02\n",
      " 4.1958e-03\n",
      " 2.1713e-03\n",
      "-7.5023e-02\n",
      "-5.7235e-02\n",
      "-8.0981e-02\n",
      " 6.4885e-03\n",
      "-5.9523e-03\n",
      "-2.3526e-02\n",
      " 1.0603e-02\n",
      "-5.7543e-02\n",
      "-8.3219e-02\n",
      "-7.2507e-03\n",
      "-8.0146e-03\n",
      "-4.7321e-02\n",
      "-6.8038e-02\n",
      "-8.1712e-02\n",
      "-2.9335e-02\n",
      "-5.3847e-03\n",
      "-4.9628e-02\n",
      "-2.0973e-02\n",
      "-4.9873e-03\n",
      "-3.7319e-02\n",
      "-9.0278e-02\n",
      "-1.2156e-03\n",
      "-3.2882e-03\n",
      "-1.3226e-02\n",
      " 3.8202e-03\n",
      "-6.4074e-03\n",
      "-3.4338e-02\n",
      "-3.5431e-02\n",
      "-7.1698e-02\n",
      " 1.1170e-02\n",
      "-6.0093e-02\n",
      " 5.4725e-03\n",
      " 1.3168e-02\n",
      " 1.3976e-02\n",
      "-6.2790e-02\n",
      " 8.9646e-03\n",
      "-7.0408e-02\n",
      " 2.9531e-03\n",
      " 1.3327e-02\n",
      "-3.7236e-02\n",
      "-3.3884e-02\n",
      "-2.6341e-02\n",
      "-3.3167e-02\n",
      "-5.5715e-04\n",
      "-1.6162e-02\n",
      "-1.9365e-02\n",
      "-6.6530e-02\n",
      "-1.6064e-02\n",
      "-3.7513e-02\n",
      "-2.5951e-02\n",
      "-7.5553e-02\n",
      "-1.4394e-02\n",
      "-7.6160e-02\n",
      "-8.3181e-02\n",
      "-4.3638e-03\n",
      "-3.7430e-02\n",
      "-5.2084e-02\n",
      "-1.9853e-02\n",
      "-6.3424e-02\n",
      " 6.2004e-03\n",
      "-1.4473e-02\n",
      "-8.4158e-02\n",
      "-5.7680e-02\n",
      "-6.4924e-02\n",
      "-4.4710e-02\n",
      "-8.1016e-02\n",
      "-6.2942e-02\n",
      "-6.8119e-02\n",
      "-5.5461e-02\n",
      "-3.3067e-02\n",
      "-2.4773e-02\n",
      "-7.4272e-02\n",
      "-1.8021e-02\n",
      "-5.0231e-02\n",
      "-8.2263e-02\n",
      "-3.0140e-02\n",
      "-3.2495e-02\n",
      "-1.7679e-02\n",
      "-2.8932e-02\n",
      "-4.4588e-02\n",
      "-8.0571e-02\n",
      "-4.4514e-02\n",
      " 2.1628e-03\n",
      "-7.6708e-02\n",
      "-4.5084e-02\n",
      "-6.3922e-02\n",
      "-8.5928e-02\n",
      "-7.1128e-02\n",
      " 3.7463e-03\n",
      "-1.9894e-02\n",
      "-7.9914e-03\n",
      "-6.7699e-02\n",
      " 6.4629e-03\n",
      "-4.8915e-02\n",
      " 1.8496e-02\n",
      "-3.7544e-02\n",
      "-1.6196e-02\n",
      "-3.3968e-02\n",
      "-8.1880e-02\n",
      " 8.0240e-03\n",
      "-7.4277e-02\n",
      "-4.1738e-02\n",
      "-3.0165e-02\n",
      "-2.1300e-02\n",
      "-6.5857e-03\n",
      "-7.7553e-02\n",
      "-1.8607e-02\n",
      "-7.8503e-02\n",
      "-4.3203e-02\n",
      "-8.5745e-02\n",
      " 6.7959e-03\n",
      "-9.8754e-03\n",
      "-7.4540e-02\n",
      "-6.7486e-02\n",
      "-8.2920e-02\n",
      "-1.2871e-02\n",
      "-8.4978e-02\n",
      " 1.0352e-02\n",
      " 8.3001e-06\n",
      "-5.3219e-02\n",
      "-3.3803e-02\n",
      "-7.0417e-02\n",
      "-3.0546e-02\n",
      " 1.6479e-02\n",
      "-1.2599e-02\n",
      "-1.3416e-02\n",
      "-7.6249e-02\n",
      "-6.4138e-02\n",
      "-3.1927e-02\n",
      "-7.7266e-03\n",
      "-7.6049e-02\n",
      "-2.4901e-02\n",
      "-6.6843e-02\n",
      "-8.9090e-02\n",
      "-2.9406e-02\n",
      "-9.0235e-02\n",
      "-6.8687e-02\n",
      "-3.2450e-02\n",
      "-4.1755e-02\n",
      "-3.3069e-02\n",
      "-5.3197e-02\n",
      "-6.8314e-02\n",
      " 1.4111e-02\n",
      "-6.4062e-02\n",
      "-6.5136e-02\n",
      "-4.9290e-02\n",
      " 1.1864e-02\n",
      "-5.2686e-03\n",
      " 2.0479e-02\n",
      "-1.0652e-02\n",
      "-4.1628e-02\n",
      " 1.8339e-02\n",
      "-7.2029e-02\n",
      " 1.4706e-02\n",
      "-4.8930e-02\n",
      "-8.1565e-03\n",
      "-3.2373e-03\n",
      "-9.3106e-02\n",
      "-4.4786e-02\n",
      "-3.9826e-02\n",
      "-6.3060e-02\n",
      "-4.1726e-02\n",
      "-6.4509e-02\n",
      "-7.9855e-02\n",
      "-8.8646e-02\n",
      " 9.6674e-03\n",
      "-6.2429e-02\n",
      "-3.2535e-02\n",
      "-1.5059e-02\n",
      "-4.3850e-02\n",
      " 7.1309e-03\n",
      "-7.2611e-02\n",
      "-2.6042e-02\n",
      "-6.2703e-02\n",
      "-5.2188e-02\n",
      "-3.8468e-02\n",
      " 1.7705e-03\n",
      "-5.9820e-02\n",
      "-7.8778e-03\n",
      "-3.8955e-02\n",
      "-1.1669e-03\n",
      "-1.1743e-02\n",
      "-4.6868e-02\n",
      "-4.3178e-02\n",
      "-6.7895e-02\n",
      "-5.1946e-02\n",
      "-8.5593e-02\n",
      "-4.3517e-02\n",
      "-6.3839e-02\n",
      "-8.5118e-02\n",
      "-3.1790e-02\n",
      "-1.3810e-02\n",
      "-4.4361e-03\n",
      "-1.6534e-02\n",
      "-4.9910e-02\n",
      "-3.9625e-02\n",
      "-4.9545e-02\n",
      "-3.0949e-02\n",
      "-2.8775e-02\n",
      "-6.1762e-02\n",
      "-8.4736e-02\n",
      "-7.6188e-02\n",
      "-2.4491e-04\n",
      "-4.5239e-02\n",
      "-3.2041e-02\n",
      "-3.5755e-02\n",
      " 1.6588e-02\n",
      " 1.0247e-02\n",
      "-1.1617e-02\n",
      "-2.3972e-03\n",
      "-6.0697e-02\n",
      "-6.6021e-02\n",
      "-7.8754e-02\n",
      "-4.3996e-02\n",
      "-1.0263e-01\n",
      "-1.1585e-01\n",
      "-1.0402e-01\n",
      "-7.0803e-02\n",
      "-2.8589e-02\n",
      "-4.4189e-04\n",
      "-4.7792e-02\n",
      "-4.7598e-02\n",
      "-2.5286e-03\n",
      "-6.6770e-03\n",
      "-1.5782e-02\n",
      "-3.1278e-02\n",
      " 5.0936e-03\n",
      "-7.4145e-02\n",
      "-2.3289e-02\n",
      " 2.1232e-03\n",
      "-7.6933e-02\n",
      " 8.9486e-03\n",
      "-6.3158e-02\n",
      " 2.0971e-02\n",
      " 1.4962e-02\n",
      "-7.1944e-02\n",
      "-8.0841e-02\n",
      "-6.7172e-02\n",
      "-6.3744e-02\n",
      "-6.9902e-02\n",
      "-4.9493e-02\n",
      "-6.9032e-02\n",
      "-3.5496e-02\n",
      "-1.0419e-01\n",
      "-7.3915e-02\n",
      "-5.6070e-02\n",
      "-8.9555e-02\n",
      "-5.9643e-02\n",
      "-2.6864e-02\n",
      "-6.4071e-02\n",
      " 7.0592e-03\n",
      "-2.2677e-03\n",
      "-3.1890e-02\n",
      "-6.5069e-02\n",
      "-2.4917e-03\n",
      "-6.5055e-02\n",
      "-1.8196e-02\n",
      "-6.0133e-02\n",
      "-2.6996e-02\n",
      " 2.4191e-03\n",
      "-4.0331e-02\n",
      "-5.2537e-02\n",
      "-2.3347e-02\n",
      "-5.2090e-02\n",
      "-5.3256e-02\n",
      "-1.4818e-02\n",
      "-5.2373e-02\n",
      "-2.3535e-02\n",
      "-7.6793e-02\n",
      "-1.6949e-02\n",
      "-8.4705e-02\n",
      "-7.8088e-02\n",
      "-9.7429e-02\n",
      "-6.5970e-02\n",
      "-4.9836e-03\n",
      "-5.6593e-02\n",
      "-6.3801e-04\n",
      "-5.4939e-02\n",
      "-7.1086e-02\n",
      " 2.0249e-02\n",
      "-2.7291e-02\n",
      " 1.5003e-02\n",
      "-7.4715e-02\n",
      "-4.1885e-02\n",
      "-7.7183e-02\n",
      "-6.4197e-02\n",
      "-8.0929e-02\n",
      "-1.1836e-02\n",
      "-7.3141e-02\n",
      "-1.5568e-02\n",
      "-1.2204e-02\n",
      "-8.4876e-02\n",
      "-8.4494e-02\n",
      "-2.6923e-02\n",
      "-2.9089e-02\n",
      "-6.3957e-02\n",
      "-2.3522e-02\n",
      "-1.5680e-02\n",
      "-9.1984e-02\n",
      "-9.5790e-02\n",
      "-6.7213e-02\n",
      "-7.3371e-02\n",
      "-4.2729e-02\n",
      "-1.7427e-02\n",
      "-2.5905e-02\n",
      "-2.1194e-02\n",
      "-2.2913e-03\n",
      "-5.3574e-02\n",
      " 1.4064e-02\n",
      "-3.3539e-02\n",
      " 1.3497e-02\n",
      "-7.9026e-02\n",
      "-3.1592e-03\n",
      "-2.6130e-02\n",
      " 6.2509e-03\n",
      "-1.2688e-02\n",
      "-1.9994e-02\n",
      "-7.0158e-02\n",
      "-7.8110e-02\n",
      "-4.1198e-02\n",
      "-3.1893e-02\n",
      "-2.4427e-02\n",
      "-8.9536e-02\n",
      "-4.5965e-02\n",
      " 1.0501e-02\n",
      "-8.6491e-02\n",
      "-8.3805e-02\n",
      "-8.6247e-02\n",
      "-4.1363e-02\n",
      "-3.5473e-02\n",
      "-3.7063e-02\n",
      "-7.3627e-03\n",
      "-4.3475e-03\n",
      "-6.1256e-03\n",
      "-2.1676e-02\n",
      "-2.0110e-03\n",
      "-4.1696e-02\n",
      "-4.8649e-03\n",
      "-6.5749e-02\n",
      "-4.9617e-02\n",
      "-5.2250e-02\n",
      "-6.1347e-02\n",
      "-6.0549e-02\n",
      " 1.2593e-02\n",
      "-5.2447e-02\n",
      "-1.1489e-02\n",
      "-1.2838e-02\n",
      "-1.6255e-02\n",
      "-4.7865e-02\n",
      "-5.9639e-02\n",
      "-6.0350e-02\n",
      "-6.6098e-02\n",
      "-1.6504e-02\n",
      "-1.8018e-02\n",
      "-6.2569e-02\n",
      "-2.6544e-03\n",
      "-3.6448e-02\n",
      " 2.5652e-03\n",
      " 4.7839e-03\n",
      "-3.6712e-02\n",
      " 9.7265e-03\n",
      "-1.4736e-02\n",
      "-1.2837e-02\n",
      "-8.4704e-03\n",
      "-2.9928e-02\n",
      "-4.4244e-02\n",
      "-6.1094e-02\n",
      "-3.2140e-02\n",
      "-3.6091e-02\n",
      "-3.1438e-02\n",
      "-5.1672e-02\n",
      "-5.7336e-03\n",
      "-4.2093e-02\n",
      "-3.8404e-02\n",
      "-5.8109e-02\n",
      " 3.0324e-03\n",
      "-6.9125e-02\n",
      "-6.8798e-02\n",
      "-7.0155e-02\n",
      "-7.8081e-02\n",
      "-8.0725e-02\n",
      "-4.9566e-02\n",
      "-8.0336e-02\n",
      "-5.3981e-02\n",
      "-4.9330e-02\n",
      " 4.6020e-03\n",
      "-8.2934e-02\n",
      "-2.4725e-02\n",
      "-1.5433e-02\n",
      "-2.0012e-02\n",
      "-1.5784e-02\n",
      "-1.1495e-02\n",
      "-2.2743e-03\n",
      "-5.7345e-02\n",
      "-4.5806e-02\n",
      "-1.9631e-02\n",
      "-5.6525e-02\n",
      "-3.0736e-02\n",
      "-6.1235e-03\n",
      "-7.5968e-02\n",
      "-4.9446e-02\n",
      "-1.6026e-02\n",
      "-7.8988e-03\n",
      "-2.6789e-02\n",
      "-8.0162e-02\n",
      "-3.9292e-02\n",
      "-1.3578e-02\n",
      "-1.6329e-02\n",
      "-7.8907e-02\n",
      "-5.5128e-02\n",
      "-2.0305e-02\n",
      "-8.6858e-03\n",
      "-6.1506e-02\n",
      "-2.5537e-02\n",
      "-3.8791e-02\n",
      " 1.9089e-02\n",
      "-3.8943e-03\n",
      "-6.1671e-02\n",
      " 8.7199e-03\n",
      "-6.8717e-02\n",
      "-8.4936e-03\n",
      "-4.7352e-02\n",
      "-4.8287e-02\n",
      " 1.0929e-02\n",
      "-9.6255e-03\n",
      "-6.7724e-02\n",
      "-1.4415e-02\n",
      "-2.1874e-02\n",
      "-1.5012e-02\n",
      "-6.0912e-02\n",
      "-1.9399e-02\n",
      " 2.2935e-02\n",
      "-6.9848e-02\n",
      "-3.7785e-02\n",
      "-1.2218e-02\n",
      " 1.4617e-02\n",
      "-1.6855e-02\n",
      "-4.1356e-02\n",
      "-7.9608e-03\n",
      "-3.4430e-02\n",
      "-7.1253e-02\n",
      "-5.8739e-02\n",
      "-1.8748e-02\n",
      "-7.2780e-02\n",
      " 1.2928e-02\n",
      " 1.1370e-02\n",
      "-5.1291e-02\n",
      "-1.7733e-02\n",
      " 1.2894e-02\n",
      "-5.1528e-02\n",
      "-3.7700e-02\n",
      "-5.8135e-02\n",
      "-1.7483e-02\n",
      "-1.9561e-04\n",
      " 7.2616e-03\n",
      " 9.9177e-03\n",
      "-2.1237e-02\n",
      " 2.1709e-02\n",
      " 1.6471e-02\n",
      "-6.3260e-02\n",
      "-5.2191e-02\n",
      "-1.1184e-02\n",
      "-7.3862e-02\n",
      "-6.7193e-02\n",
      "-5.5775e-02\n",
      "-1.0344e-01\n",
      "-1.4668e-02\n",
      "-4.6975e-02\n",
      "-2.0459e-02\n",
      "-5.8492e-02\n",
      "-4.0068e-02\n",
      "-6.4305e-02\n",
      "-7.2893e-03\n",
      "-5.2138e-02\n",
      " 3.7093e-03\n",
      " 2.0408e-02\n",
      "-7.7360e-02\n",
      "-7.9534e-02\n",
      "-7.7792e-02\n",
      " 5.1447e-03\n",
      "-3.6929e-02\n",
      "-2.3844e-02\n",
      "-5.1496e-02\n",
      "-5.7490e-02\n",
      " 1.1386e-02\n",
      "-1.5348e-03\n",
      "-4.7868e-04\n",
      "-2.9278e-02\n",
      "-2.2560e-02\n",
      "-1.9724e-02\n",
      "-3.4605e-02\n",
      "-2.9745e-02\n",
      "-9.3512e-02\n",
      "-9.3733e-02\n",
      "-2.8710e-02\n",
      "-5.2359e-02\n",
      "-1.0874e-01\n",
      "-5.8926e-02\n",
      "-4.3554e-02\n",
      "-4.0129e-02\n",
      " 3.0181e-04\n",
      "-2.1753e-02\n",
      "-5.2515e-02\n",
      " 1.2254e-02\n",
      " 1.3266e-02\n",
      "-1.2678e-02\n",
      "-6.7081e-02\n",
      "-6.9058e-02\n",
      "-5.8842e-02\n",
      "-6.5951e-02\n",
      "-2.5897e-03\n",
      " 3.7762e-03\n",
      "-2.1068e-02\n",
      "-5.0118e-02\n",
      "-2.8129e-04\n",
      "-2.5529e-02\n",
      " 7.4367e-03\n",
      " 1.3462e-03\n",
      " 6.0041e-05\n",
      "-6.9737e-02\n",
      "-9.1979e-02\n",
      "-5.4088e-02\n",
      "-6.2569e-02\n",
      "-7.3916e-02\n",
      "-1.0488e-01\n",
      "-9.1603e-02\n",
      "-1.5968e-02\n",
      "-7.5913e-02\n",
      "-3.9373e-02\n",
      "-3.8344e-02\n",
      "-8.4147e-03\n",
      "-2.0931e-02\n",
      "-2.5099e-02\n",
      "-7.3989e-02\n",
      "-3.9743e-02\n",
      "-1.3030e-02\n",
      " 1.2327e-02\n",
      " 9.0017e-03\n",
      "-2.3337e-04\n",
      "-7.0971e-02\n",
      "-3.3277e-02\n",
      "-6.1304e-02\n",
      "-6.0801e-02\n",
      "-4.5638e-02\n",
      "-3.6837e-02\n",
      "-4.3408e-02\n",
      "-4.7756e-02\n",
      "-3.5851e-03\n",
      "-3.3707e-02\n",
      "-5.6365e-02\n",
      "-1.7886e-02\n",
      "-6.3401e-02\n",
      "-5.6603e-02\n",
      "-3.7261e-02\n",
      "-1.4415e-02\n",
      " 2.4945e-03\n",
      " 3.1190e-04\n",
      "-6.1116e-02\n",
      " 1.1122e-02\n",
      "-6.6653e-02\n",
      "-1.3223e-02\n",
      "-4.7373e-02\n",
      "-3.2167e-02\n",
      "-3.9021e-02\n",
      "-3.9492e-03\n",
      "-2.6157e-02\n",
      " 8.7477e-03\n",
      "-2.5719e-02\n",
      "-5.7365e-02\n",
      "-1.8399e-03\n",
      "-4.2542e-02\n",
      "-3.0063e-02\n",
      "-4.3642e-03\n",
      "-5.7196e-02\n",
      "-3.8364e-02\n",
      "-4.9343e-02\n",
      "-8.6766e-02\n",
      "-6.1859e-02\n",
      "-3.4473e-02\n",
      "-3.5763e-02\n",
      "-9.9968e-03\n",
      "-1.1067e-02\n",
      "-1.4553e-02\n",
      "-5.1468e-02\n",
      " 2.8613e-04\n",
      "-8.3842e-03\n",
      "-6.4582e-02\n",
      " 1.4847e-03\n",
      "-4.0746e-02\n",
      "-2.1333e-02\n",
      " 7.5619e-03\n",
      "-1.9214e-02\n",
      "-2.9810e-03\n",
      "-7.7344e-02\n",
      "-4.3218e-02\n",
      "-4.7343e-02\n",
      "-6.8139e-02\n",
      "-3.6716e-02\n",
      "-4.2753e-02\n",
      "-7.9313e-02\n",
      "-7.5083e-02\n",
      "-7.8686e-02\n",
      "-3.6240e-04\n",
      "-7.5375e-02\n",
      "-8.0328e-02\n",
      "-5.7860e-02\n",
      "-6.3709e-02\n",
      "-1.3115e-02\n",
      "-7.1753e-02\n",
      "-4.9333e-03\n",
      "-4.4780e-02\n",
      "-6.4306e-02\n",
      "-7.1354e-02\n",
      " 1.5514e-02\n",
      " 1.5624e-02\n",
      "-6.5704e-02\n",
      "-5.6433e-02\n",
      "-8.3547e-02\n",
      "-7.7282e-02\n",
      "-2.0396e-02\n",
      "-4.7851e-02\n",
      "-8.0611e-02\n",
      "-7.2142e-02\n",
      "-5.3376e-02\n",
      "-3.1745e-03\n",
      "-7.0718e-02\n",
      "-3.2091e-02\n",
      "-5.3834e-02\n",
      "-6.9380e-02\n",
      " 6.7950e-03\n",
      "-1.1839e-02\n",
      "-4.8383e-02\n",
      "-3.4042e-02\n",
      "-6.7114e-02\n",
      "-2.5013e-02\n",
      "-6.8030e-02\n",
      "-8.6366e-02\n",
      "-1.9681e-02\n",
      "-6.0115e-02\n",
      "-4.1202e-02\n",
      "-5.6919e-02\n",
      "-6.3809e-02\n",
      "-2.9973e-02\n",
      "-7.6234e-02\n",
      " 1.3071e-02\n",
      "-4.9604e-02\n",
      "-1.2234e-02\n",
      " 9.1268e-03\n",
      "-6.0076e-02\n",
      "-6.7728e-02\n",
      " 7.3353e-03\n",
      " 2.1670e-03\n",
      " 1.1634e-03\n",
      "-5.9104e-03\n",
      "-7.5861e-02\n",
      "-5.9255e-02\n",
      "-1.0025e-02\n",
      "-8.4868e-02\n",
      " 8.5613e-03\n",
      "-1.5199e-02\n",
      "-2.0155e-02\n",
      "-2.9016e-02\n",
      "-6.0253e-02\n",
      "-8.1488e-02\n",
      "-2.7955e-02\n",
      "-2.7349e-02\n",
      "-3.8585e-02\n",
      "-9.4500e-03\n",
      "-6.3615e-02\n",
      "-5.8180e-02\n",
      "-5.5239e-02\n",
      "-1.8297e-02\n",
      "-2.8180e-02\n",
      "-6.9735e-02\n",
      "-5.4283e-02\n",
      "-5.6556e-03\n",
      "-4.9381e-02\n",
      "-7.1266e-03\n",
      "-1.7148e-02\n",
      "-4.4470e-02\n",
      " 1.3245e-02\n",
      "-7.9403e-02\n",
      "-6.5259e-02\n",
      "-4.2391e-02\n",
      "-4.3270e-02\n",
      "-7.9206e-02\n",
      "-6.1689e-02\n",
      "-2.7799e-02\n",
      "-7.6832e-02\n",
      "-3.0086e-02\n",
      "-5.4532e-02\n",
      " 4.1117e-03\n",
      "-2.4733e-02\n",
      "-2.9171e-02\n",
      "-7.7038e-04\n",
      "-7.5323e-02\n",
      "-2.3575e-02\n",
      " 1.5265e-02\n",
      " 2.1859e-03\n",
      "-3.1121e-02\n",
      "-4.0194e-02\n",
      "-7.2069e-02\n",
      "-5.3340e-02\n",
      "-5.8346e-02\n",
      "-5.3840e-02\n",
      "-2.2776e-02\n",
      "-4.2781e-02\n",
      "-6.6740e-02\n",
      "-3.0300e-02\n",
      "-7.9434e-02\n",
      "-7.1144e-02\n",
      "-1.8506e-02\n",
      "-7.2853e-02\n",
      "-6.7960e-02\n",
      "-6.7223e-02\n",
      "-4.4499e-02\n",
      "-4.5327e-02\n",
      "-2.8974e-02\n",
      "-4.5741e-02\n",
      "-2.4146e-02\n",
      "-3.2897e-02\n",
      "-2.1004e-02\n",
      "-6.1202e-02\n",
      "-6.8869e-02\n",
      "-2.8787e-02\n",
      "-7.2140e-02\n",
      "-6.1815e-02\n",
      "-1.1169e-02\n",
      " 4.8203e-03\n",
      " 8.1969e-03\n",
      "-7.3660e-02\n",
      "-1.9615e-02\n",
      "[torch.FloatTensor of size 784]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for tag, value in model.named_parameters():\n",
    "    print(tag)\n",
    "    print(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20717.869140625"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss.data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NLLLoss' object has no attribute 'data'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-fb7bcca8bf25>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/mappiness/anaconda3/envs/torch/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    236\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0;32m--> 238\u001b[0;31m             type(self).__name__, name))\n\u001b[0m\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NLLLoss' object has no attribute 'data'"
     ]
    }
   ],
   "source": [
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "dimension 1 out of range of 1D tensor at /py/conda-bld/pytorch_1493681908901/work/torch/lib/TH/generic/THTensor.c:24",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-992665bb96ed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecon_batch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecon_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/mappiness/anaconda3/envs/torch/lib/python3.6/site-packages/torch/autograd/variable.py\u001b[0m in \u001b[0;36mcat\u001b[0;34m(iterable, dim)\u001b[0m\n\u001b[1;32m    839\u001b[0m         \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    840\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 841\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mConcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0miterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    842\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    843\u001b[0m         \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mappiness/anaconda3/envs/torch/lib/python3.6/site-packages/torch/autograd/_functions/tensor.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *inputs)\u001b[0m\n\u001b[1;32m    307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 309\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_sizes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    310\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mappiness/anaconda3/envs/torch/lib/python3.6/site-packages/torch/autograd/_functions/tensor.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 309\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_sizes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    310\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: dimension 1 out of range of 1D tensor at /py/conda-bld/pytorch_1493681908901/work/torch/lib/TH/generic/THTensor.c:24"
     ]
    }
   ],
   "source": [
    "torch.cat((recon_batch,Variable(torch.ones(len(recon_batch)))),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tmp = recon_batch.view(100,784,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'torch.FloatTensor' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-7b6cdee36fdb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'torch.FloatTensor' object is not callable"
     ]
    }
   ],
   "source": [
    "tmp.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "torch.cat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recon_batch.size()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_value:0.8387617468833923\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "expected a Variable argument, but got torch.FloatTensor",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-3960868de0fe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"loss_value:{}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_value\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0;31m#we need 3 dimension for tensor to visualize it!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m             \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabel_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_img\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mappiness/anaconda3/envs/torch/lib/python3.6/site-packages/torch/autograd/variable.py\u001b[0m in \u001b[0;36mcat\u001b[0;34m(iterable, dim)\u001b[0m\n\u001b[1;32m    839\u001b[0m         \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    840\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 841\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mConcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0miterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    842\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    843\u001b[0m         \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: expected a Variable argument, but got torch.FloatTensor"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:torch]",
   "language": "python",
   "name": "conda-env-torch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
